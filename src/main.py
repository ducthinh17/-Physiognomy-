
import os
import io
import json
import traceback
import datetime
from zipfile import ZipFile, ZIP_DEFLATED

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse
import torch
from PIL import Image

# Import c√°c th√†nh ph·∫ßn t·ª´ file face_analyzer trong c√πng th∆∞ m·ª•c src
from .face_analyzer import (
    process_image_pipeline,
    YOLO, AutoDetectionModel,
    FEATURE_CONF_THRESHOLD
)

# --- CONFIG & MODEL LOADING ---

# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn th∆∞ m·ª•c `src` n∆°i file n√†y ƒëang ch·∫°y
current_dir = os.path.dirname(os.path.abspath(__file__))
# ƒêi ng∆∞·ª£c l√™n m·ªôt c·∫•p (t·ªõi th∆∞ m·ª•c g·ªëc c·ªßa project), sau ƒë√≥ v√†o th∆∞ m·ª•c `models`
MODEL_DIR = os.path.join(current_dir, '..', 'models')

# T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß t·ªõi c√°c file model
FACE_MODEL_PATH = os.path.join(MODEL_DIR, 'only_face.pt')
FEATURE_MODEL_PATH = os.path.join(MODEL_DIR, 'Face_detection.pt')
CLASSIFICATION_MODEL_PATH = os.path.join(MODEL_DIR, 'model_85_face_types.pth')

app = FastAPI(title="Face Analysis API", description="API for facial feature and proportion analysis.")
models = {} # D√πng dictionary ƒë·ªÉ l∆∞u c√°c model ƒë√£ load

@app.on_event("startup")
def load_models():
    """T·∫£i t·∫•t c·∫£ c√°c model v√†o b·ªô nh·ªõ khi server kh·ªüi ƒë·ªông."""
    print("--- üöÄ Server starting up, loading models... ---")

    # Ki·ªÉm tra xem c√°c file model c√≥ t·ªìn t·∫°i kh√¥ng
    if not all(os.path.exists(p) for p in [FACE_MODEL_PATH, FEATURE_MODEL_PATH, CLASSIFICATION_MODEL_PATH]):
        print(f"FATAL: One or more model files not found in {MODEL_DIR}. Please check your project structure.")
        return

    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")

        # Load c√°c model v√† l∆∞u v√†o dictionary
        models['face_detector'] = YOLO(FACE_MODEL_PATH)
        models['feature_detector'] = AutoDetectionModel.from_pretrained(
            model_type='yolov8', model_path=FEATURE_MODEL_PATH,
            confidence_threshold=FEATURE_CONF_THRESHOLD, device=device
        )
        models['face_shape_classifier'] = torch.load(CLASSIFICATION_MODEL_PATH, map_location=device, weights_only=False)
        models['face_shape_classifier'].to(device).eval()

        print("--- ‚úÖ All models loaded successfully! Server is ready. ---")
    except Exception as e:
        print(f"FATAL ERROR during model loading: {e}")
        traceback.print_exc()

# --- API ENDPOINTS ---

@app.get("/", summary="Health Check")
def read_root():
    """Endpoint ƒë·ªÉ ki·ªÉm tra server c√≥ ƒëang ho·∫°t ƒë·ªông kh√¥ng."""
    return {
        "status": "online",
        "timestamp": datetime.datetime.now().isoformat(),
        "models_loaded": "face_detector" in models
    }

@app.post("/analyze-face/", summary="Analyze a single face image")
async def analyze_face(file: UploadFile = File(..., description="Image file (jpg, png) to analyze.")):
    """
    Nh·∫≠n m·ªôt file ·∫£nh, x·ª≠ l√Ω v√† tr·∫£ v·ªÅ m·ªôt file ZIP ch·ª©a 3 file k·∫øt qu·∫£:
    - `annotated_image.jpg`: ·∫¢nh g·ªëc v·ªõi c√°c ƒë∆∞·ªùng k·∫ª ph√¢n t√≠ch.
    - `report.jpg`: ·∫¢nh report t·ªïng h·ª£p.
    - `analysis.json`: D·ªØ li·ªáu ph√¢n t√≠ch chi ti·∫øt d·∫°ng JSON.
    """
    if not models:
        raise HTTPException(status_code=503, detail="Models are not loaded. Server is not ready.")

    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="Invalid file type. Please upload an image file.")

    image_bytes = await file.read()

    print(f"Processing image: {file.filename}, size: {len(image_bytes)} bytes")

    annotated_img, report_img, master_json_data = process_image_pipeline(
        image_bytes=image_bytes,
        filename=file.filename,
        face_detector=models['face_detector'],
        feature_detector=models['feature_detector'],
        face_shape_classifier=models['face_shape_classifier']
    )

    if annotated_img is None:
        error_detail = master_json_data.get("error", "Failed to process image.")
        print(f"Analysis failed for {file.filename}: {error_detail}")
        raise HTTPException(status_code=422, detail=error_detail)

    # T·∫°o file ZIP trong b·ªô nh·ªõ ƒë·ªÉ tr·∫£ v·ªÅ cho ng∆∞·ªùi d√πng
    zip_buffer = io.BytesIO()
    with ZipFile(zip_buffer, 'a', ZIP_DEFLATED, False) as zip_file:
        # L∆∞u ·∫£nh ch√∫ th√≠ch
        img_byte_arr = io.BytesIO()
        annotated_img.save(img_byte_arr, format='JPEG', quality=95)
        zip_file.writestr('annotated_image.jpg', img_byte_arr.getvalue())

        # L∆∞u ·∫£nh report
        report_byte_arr = io.BytesIO()
        report_img.save(report_byte_arr, format='JPEG', quality=95)
        zip_file.writestr('report.jpg', report_byte_arr.getvalue())

        # L∆∞u file JSON
        json_str = json.dumps(master_json_data, indent=4, ensure_ascii=False)
        zip_file.writestr('analysis.json', json_str.encode('utf-8'))

    zip_buffer.seek(0)

    print(f"Successfully processed {file.filename}. Returning zip file.")

    return StreamingResponse(
        zip_buffer,
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename=result_{file.filename}.zip"}
    )